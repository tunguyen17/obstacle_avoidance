{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Center> \n",
    "    <h1> M5366: Datamining II - Final Project </h1>\n",
    "    <h3>Dr. Scott Cook</h3>\n",
    "    <h2>Using Neural Network to Train Self-Driving Car</h2>\n",
    "    <h3> Tu A. Nguyen </h3>\n",
    "    \n",
    "</Center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Abstract\n",
    "\n",
    "Inspired by biological processes, neural network is considered as one of the best universal estimator for datamining and machine learning. In 2015, [DeepMind](https://deepmind.com/) published a research paper that describe a system called deep Q-network (DQN). DQN uses neural network to train computer program to play Atari 2600 games. In this project, we seek to develop a self-driving car using DQN. We use computer simulation to train  an agent to navigate a car though a 2D environment. The car has a set of distance sensors that provides the agent with the information about the environment. The main task is to avoid crashing into the objects in the environment. After training, the agent was able to navigate through both static and dynamic environment without crashing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Background\n",
    "\n",
    "## 1. Artificial Neural Network (ANN)\n",
    "\n",
    "An ANN consists of a sets of artificual neuron connected together. The input signals are processed by these neuron collectively in order to produce the output signals. As the ANN learn by changing the weights between neurons as it recieved feedback data during the trainning process. \n",
    "\n",
    "<img src=\"imgs/ann.png\" alt=\"ann\" style=\"width: 600px;\"/>\n",
    "[source](https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1)\n",
    "\n",
    "\n",
    "## 2. Q-learning\n",
    "\n",
    "In order to understand Q-learning, we need to first understand Markov Decision Process (MDP).\n",
    "\n",
    "### Markov Decision Process (MDP)\n",
    "\n",
    "MDP is a framework to find the best sequence of actions to perform when the outcome of each action is non-deterministic. In this case, the non-determinism must be $1^{st}$ order Markov which means that given the present state, the future states are independent from the past states.\n",
    "\n",
    "$$P(s^{t+1} | s^t, a^t) = P(s^{t+1} | s^t, a^t, s^{t-1}, a^{t-1}, \\dots, s^1, a^1, s^0)$$\n",
    "\n",
    "MDPs Problem is formally defnied as 4-tuples $(S, A, T, R)$, where\n",
    "\n",
    "- $S$ represents the state space, which is all the possible states\n",
    "\n",
    "- $A$ represents the action space, which is all the possible actions \n",
    "\n",
    "- $T(s, a, s') = P(S_{t+1} = s' | S_t = s, A_t = a), \\text{ for some } s, s' \\in S \\text{ and } a \\in A$$\n",
    "\n",
    "- $R$ represents reward function,\n",
    "\n",
    "$$R(s, a): (S, A) \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "\n",
    "In a MDP problem, a policy ($\\pi$) is defined as a mapping from states to actions $\\pi: S \\rightarrow A$. In other words, any states s in S, $\\pi(s)$ will tell us what action the agent should perform. In short, policy can be underststand as the strategy in any given state.\n",
    "\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "Q-learning is a reinforecemnt learning technique. The Q-value is defined as the cumulative discounted reward of doing action $a$ in state $s$ and the following the optimal policy. Q-learning uses temporal differences to estimate the value of $Q^*(s,a)$. In particular, an experience (s, a, r, s') provides one data point for the value Q(s,a). Then, the Q-value is updated using the folowing formula. \n",
    "\n",
    "![](imgs/qlearning.svg)\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Q-learning)\n",
    "\n",
    "### Q-learning and The Curse of Dimentionality\n",
    "\n",
    "In practice, the Q-value of each state $s$ and action $a$ is store in an $n\\times m$ table, where $|S| = n$ and $|A| = m$. If the state space and action space are small, the time it takes to estimate all the Q-value in the table would me small. Nonetheless, as the size of the state space and action space grow, the size of the table would grow. Thus, the trainning process will be significantly slower as the state space and action space grows. Furthermore, since Q-learning state space and action space are usually discrete spaces, problem involving continuous data or actions might requires the discretization of the spaces. Another alternative is to use neural network as an estimator for the Q-value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Simulation Design\n",
    "\n",
    "## 1. Environment\n",
    "\n",
    "- Firstly, our simulation is considered as a **continuous environment** since the data returned by the sensors are real numbers.\n",
    "- Secondly, this is a **deterministic environment** since for each state the sensors will always return the exact and correct distance values. \n",
    "- Thirdly, the environment is also **partially observable**, since the car cannot see the whole environment using its sensors. \n",
    "- Lastly, in this project we will have both **statics and dynamics environment**.\n",
    "\n",
    "## 2. Agent\n",
    "\n",
    "The goal of reinforcement learning is to train an agent to autonomously make decision. In particular, an agent is a computer program that\n",
    "\n",
    "- Gather information about an environment, and\n",
    "- Takes actions autonomously based on that information\n",
    "\n",
    "Each agent has 6 components action space (A), percept space (O), state space (S), world dynamics (T), perception function (Z), and utility function (U).  \n",
    "\n",
    "### Action Space (A)\n",
    "\n",
    "In this project, the action spaces consist of \n",
    "\n",
    "- 0 : Turn Left (The car turn $10^\\circ$ counter clockwise)\n",
    "- 1 : Keep Straight\n",
    "- 2 : Turn Right (The car turn $10^\\circ$ clockwise)\n",
    "\n",
    "![Action Space](imgs/actionspace.png)\n",
    "\n",
    "### Percept Space (0)\n",
    "\n",
    "As shown above, the car has a set of 5 sensors. Each of each sensor return a value from 0 to 50. Thus, the percept space can be represented as a tuple\n",
    "\n",
    "$$\\large{(o_0, o_1, o_2, o_3, o_4)}$$\n",
    "\n",
    "such that $o_i \\in \\mathbb{R}$ and $ 0 \\leq s_i \\leq 50$ for all $0 \\leq i \\leq 4$.\n",
    "\n",
    "![State Space](imgs/statespace.png)\n",
    "\n",
    "### State Space (S)\n",
    "\n",
    "Before inputing into the neural network, we rescale the data from the percept space so that the values have the range from 0 to 1. Thus, the percept space \n",
    "\n",
    "the percept space can be represented as a tuple\n",
    "\n",
    "$$\\large{s =  (s_0, s_1, s_2, s_3, s_4)}$$\n",
    "\n",
    "such that $s_i \\in \\mathbb{R}$ and $ 0 \\leq s_i \\leq 1$ for all $0 \\leq i \\leq 4$.\n",
    "\n",
    "### Word Dynamics (T) \n",
    "\n",
    "In general, word dynamics is defined as \n",
    "\n",
    "$$T: S \\times A \\rightarrow S.$$\n",
    "\n",
    "In this project, after the agent perfrom each action, the agent will take new sensors readings.\n",
    "\n",
    "### Perception Function (Z)\n",
    "\n",
    "In general, perception function is defined as\n",
    "\n",
    "$$Z: S \\rightarrow O$$\n",
    "\n",
    "In this case, perception function is \n",
    "\n",
    "$$f(s) = \\frac{s}{50}.$$\n",
    "\n",
    "The purpose of this perception function is to rescale the input data. \n",
    "\n",
    "### Utility Function (U)\n",
    "\n",
    "In general, ultility function is defined as\n",
    "\n",
    "$$ U: S \\rightarrow  \\mathbb{R}.$$\n",
    "\n",
    "This is a function that assign a value to each states, to indicate the desirability of being in such a state with respect to the agent's task.\n",
    "\n",
    "In this project, we define the utility function as\n",
    "\n",
    "$$r(s) = \\begin{cases}\n",
    "-500 & \\text{if car crashed}\\\\\n",
    "-(50* (\\min(s)) ^ 4 + 50 & \\text{if car not crashed}\n",
    "\\end{cases}, \\text{ where $s = (s_0,s_1,s_2,s_3,s_4)$}$$\n",
    "\n",
    "Using this utility function, we can punish the agent for crashing the car. On the other hand, the agent will recieve a higher reward if the minimum value of the sensor reading is high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Q-learning implementation\n",
    "\n",
    "## 1. Neural Network \n",
    "\n",
    "We use a neural network structure to estimate the value of $Q(s, a)$. The **input** of the neural network is the scalled reading from the sensor. The neural network has 3 **output**, each corresponding to one of the Q-value for state s and action turning left (0), keep straight (1), and turn right (2). In particular, the following graph describe a general structure for a neural network that estimate the Q-value\n",
    "\n",
    "<img src=\"imgs/qann.png\" alt=\"ann\" style=\"width: 600px;\"/>\n",
    "\n",
    "Note that, in the graph above $ s = (s_0, s_1, s_2, s_3, s_4) $.\n",
    "\n",
    "The activation function for every node except the output node is sigmoid. The output nodes has linear function as the activation function. \n",
    "\n",
    "## 2. Experience Replay\n",
    "\n",
    "Since Q-learning's updates are incremental and do not converge quickly, so multiple passes with the same data could be beneficial. Thus, storing the a finite set of tuples $(s, a, r, s')$ in a buffer and train from a sample of this buffer can help to optimize the training process. \n",
    "\n",
    "## 3. Exploration vs Exploitation \n",
    "\n",
    "When trainning the agent, we also need to pay attention to the problem of exploration vs exploitation. \n",
    "\n",
    "- Exploration: is to focus on performing the action that maximize Q(s,a)\n",
    "- Exploitation: is to select a different action from the one that is currently thinks is best\n",
    "\n",
    "During the early stage of trainning, the agent usually does not have a good estimation for the Q-values. Therefore, exploration will help to update and imrpove the overall estimation. On the other hand, when the agent has a good estimation of the Q-values, it would be better to focus on exploiting the best strategy. In this project, we balance between exploration and exploitation using the $\\epsilon$-greedy alogrithm. In particular, for an $\\epsilon$ value between 0 and 1,\n",
    "\n",
    "- the agent selects the greedy action all but $\\epsilon$ of the time, and \n",
    "- the agent select random action $\\epsilon$ of the time.\n",
    "\n",
    "During our simulation loop, the $\\epsilon$ value start at 1, then for every 1000 iterations the new epsilon value is set to be $0.9*\\epsilon$. Consequently, the agent will focus on exploration at the begining and gradually move toward exploitation as it has better estimations on the Q-value.\n",
    "\n",
    "## 4. Trainning Iteration\n",
    "\n",
    "For each iteration \n",
    "\n",
    "- Collect state stata $s$\n",
    "- Input $s$ into the neural network to get the estimation for the Q-values\n",
    "- With probability $\\epsilon$ choose a random action $a$. Otherwise, select action $a$ that have the corresponding maximum Q-value. \n",
    "- Execute action $a$\n",
    "- Observe the next state $s'$ and reward $r$\n",
    "- Store the memory tuple $m$: $(s, a, r, s')$ into the buffer $D$\n",
    "- Take a random set of samples ($M$) from buffer $D$\n",
    "    - for $m_i \\in M$ set:\n",
    "    \n",
    "$$y_i = \\begin{cases}\n",
    "r & \\text{if car crashed}\\\\\n",
    "r + \\gamma \\max_{a'}Q(s', a')& \\text{if car not crashed}\n",
    "\\end{cases}$$\n",
    "\n",
    "- \n",
    "    - train the neural network where the input is $s_i$ and the target output is $y_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Results\n",
    "\n",
    "## 1. Static Environment - Train\n",
    "\n",
    "The trainning usually last about 30 minutes to an hour. In the video bellow, the neural network has 2 hidden layer, both of wich has 30 nodes. When the car can move in the static environmnent without crasing, we tested the model by put the car in a dynamic environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<center>\n",
       "<video width=\"600\" height=\"500\" controls>\n",
       "  <source src=\"videos/train.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "</center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import for display video\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<center>\n",
    "<video width=\"600\" height=\"500\" controls>\n",
    "  <source src=\"videos/train.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "</center>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Environment - Test\n",
    "\n",
    "The car can move around in a dynamic environment without crashing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<center>\n",
       "<video width=\"600\" height=\"500\" controls>\n",
       "  <source src=\"videos/test.mp4\" type=\"video/mp4\">\n",
       "</video>\n",
       "</center>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import for display video\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML(\"\"\"\n",
    "<center>\n",
    "<video width=\"600\" height=\"500\" controls>\n",
    "  <source src=\"videos/test.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "</center>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Code Structure \n",
    "\n",
    "## brain.py\n",
    "\n",
    "The learning algorithm\n",
    "\n",
    "## fun.py\n",
    "\n",
    "A helper library that provide general functions\n",
    "\n",
    "## wall.py\n",
    "\n",
    "Contain information about the obstacle\n",
    "\n",
    "## car.py\n",
    "\n",
    "Contain information about the car\n",
    "\n",
    "## Sensor.py\n",
    "\n",
    "Contain information about the sensor\n",
    "\n",
    "## graphics_train.py\n",
    "\n",
    "\n",
    "## graphics_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "http://robotics.itee.uq.edu.au/~ai/doku.php/wiki/schedule\n",
    "\n",
    "http://artint.info/html/ArtInt_265.html\n",
    "\n",
    "https://medium.com/technology-invention-and-more/everything-you-need-to-know-about-artificial-neural-networks-57fac18245a1\n",
    "\n",
    "https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loop = open('data/loop.txt', 'r')\n",
    "loop_data = loop.read()\n",
    "loop_data = loop_data.split('\\t')\n",
    "\n",
    "plt.plot([int(i) for i in loop_data[:-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "brain = open('data/brain_data', 'r')\n",
    "brain_data = brain.read()\n",
    "brain_data = brain_data.split('\\t')\n",
    "\n",
    "plt.plot([float(i) for i in brain_data[:-1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
